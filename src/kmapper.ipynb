{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb0a1ab",
   "metadata": {},
   "source": [
    "# Mapper Algorithm applied to Sentence Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283a071",
   "metadata": {},
   "source": [
    "Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89977251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from pathlib import Path\n",
    "from pandas import read_parquet, concat, DataFrame\n",
    "\n",
    "# select dataset\n",
    "dataset = \"aql\" \n",
    "if dataset in [\"aql\", \"aol\"]:\n",
    "    suffix = \"special\"\n",
    "else:\n",
    "    suffix = \"all\"\n",
    "\n",
    "# path to embeddings\n",
    "path = Path(f\"/mnt/ceph/storage/data-in-progress/data-teaching/theses/thesis-schneg/analysis_data/analysis/{dataset}-get-embeddings-{suffix}\")\n",
    "\n",
    "# get number of files in path\n",
    "files = len(list(path.glob(\"*.parquet\"))) \n",
    "print(f\"Number of files: {files}\")\n",
    "\n",
    "# set number of files to load\n",
    "numfiles = 5\n",
    "# files = None\n",
    "# load embeddings\n",
    "embeddings_data = DataFrame()\n",
    "for cnt,path in enumerate(path.glob(\"*.parquet\")):\n",
    "    print(f\"Loading {cnt+1}/{files} {path.name}\")\n",
    "    df = read_parquet(path)\n",
    "    embeddings_data = concat([embeddings_data, df], ignore_index=True)\n",
    "    # limit to files for testing\n",
    "    if cnt+1 == numfiles:\n",
    "        break\n",
    "print(embeddings_data.shape)\n",
    "print(embeddings_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71bc0c",
   "metadata": {},
   "source": [
    "Preprocess Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34188d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# change dtype of arrays in the embeddings column to float32\n",
    "embeddings_data[\"embeddings\"] = embeddings_data[\"embeddings\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "# convert to numpy array, standardize data\n",
    "embeddings = embeddings_data.to_numpy()\n",
    "\n",
    "# Stack the arrays in the embeddings column into a 2D array\n",
    "emb_array = np.stack(embeddings[:,1])\n",
    "# Standardize each feature (column-wise)\n",
    "emb_array = (emb_array - np.mean(emb_array, axis=0)) / np.std(emb_array, axis=0)\n",
    "\n",
    "print(emb_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277d87b",
   "metadata": {},
   "source": [
    "Apply Mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from kmapper.jupyter import display\n",
    "import umap\n",
    "import sklearn\n",
    "import sklearn.manifold as manifold\n",
    "\n",
    "# initialize Kepler Mapper\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "# project data into 2D subsapce via 2 step transformation, 1)isomap 2)UMAP\n",
    "# projected_data = mapper.fit_transform(emb_array, projection=[manifold.Isomap(n_components=100, n_jobs=-1), umap.UMAP(n_components=2,random_state=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40966aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster data using DBSCAN\n",
    "G = mapper.map(projected_data, emb_array, clusterer=sklearn.cluster.DBSCAN(metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an excessively long filename (helpful if saving multiple Mapper variants for single dataset)\n",
    "fileID = 'projection=' + G['meta_data']['projection'].split('(')[0] + '_' + \\\n",
    "'n_cubes=' + str(G['meta_data']['n_cubes']) + '_' + \\\n",
    "'perc_overlap=' + str(G['meta_data']['perc_overlap']) + '_' + \\\n",
    "'clusterer=' + G['meta_data']['clusterer'].split('(')[0] + '_' + \\\n",
    "'scaler=' + G['meta_data']['scaler'].split('(')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "mapper.visualize(G, \n",
    "                path_html=f\"../data/mapper_{dataset}_NumFiles_{str(files)}_{fileID}.html\",\n",
    "                title=fileID,\n",
    "                custom_tooltips = embeddings_data.iloc[:,0].to_numpy(),\n",
    "                color_function_name = 'Log Percent Returns',\n",
    "                node_color_function = np.array(['average', 'std', 'sum', 'max', 'min']))\n",
    "\n",
    "# display mapper in jupyter\n",
    "# km.jupyter.display(\"../data/mapper_example_\" + fileID + \".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# move html file to data folder\n",
    "os.system(f\"mv ../data/mapper_{dataset}_NumFiles_{str(files)}_{fileID}.html /mnt/c/Users/Benjamin/Desktop/mapper-results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
